\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{float}
\geometry{margin=1in}
\doublespacing

\title{\textbf{Prompt Sensitivity and Answer Consistency of Small
Open-Source Large Language Models on Clinical Question
Answering: Implications for Low-Resource Healthcare Deployment}}

\author{Shravani Hariprasad\\
\textit{Independent Researcher}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
To be written last.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

Artificial intelligence is increasingly being integrated into 
healthcare workflows, from clinical decision support to 
exam-style medical question answering. While much attention 
has focused on large, cloud-based models, smaller open-source 
language models are gaining importance because they can run 
locally on standard CPUs, making them viable for rural clinics, 
community hospitals, and low-resource health systems with 
limited infrastructure \citep{garg2024slm}. However, as these 
smaller models become more accessible for real-world deployment, 
questions about their reliability become critical. In clinical 
settings, even small inaccuracies can have significant 
consequences, and the safety of AI systems depends not only 
on their overall performance but on how stable and predictable 
their outputs are under different conditions. Despite rapid 
adoption, the reliability of small, locally deployable models 
in healthcare remains insufficiently studied.

Most existing evaluations of clinical language models focus 
primarily on accuracy — whether a model selects the correct 
answer on a given benchmark \citep{bedi2025systematic}. 
However, accuracy alone does not capture how stable a model's 
outputs are when the same clinical question is phrased 
differently. In safety-critical domains such as healthcare, 
this distinction matters: a model that gives different answers 
depending on how a question is worded cannot be considered 
reliable, regardless of its aggregate accuracy score.

To our knowledge, no prior study has systematically examined 
whether small, CPU-runnable models provide consistent answers 
under controlled prompt variation in clinical settings. 
The CLEVER framework \citep{cleverjmir2025} evaluates clinical 
LLM outputs through physician preference but does not assess 
response stability across prompt reformulations. Ngweta et al. 
\citep{ngweta2024robustness} quantify prompt brittleness in 
general NLP tasks but do not examine clinical datasets or 
safety implications. Kim et al. \citep{kim2025hallucination} 
demonstrate that foundation models frequently produce confident 
but incorrect medical outputs, yet do not empirically measure 
variability under prompt changes. Recent surveys on small 
language models in healthcare \citep{garg2024slm} discuss 
efficiency and deployment tradeoffs without analysing answer 
consistency as a reliability dimension. This study addresses 
that gap directly.

In this work, we systematically evaluate prompt sensitivity 
in four open-source small language models — Phi-3 Mini (3.8B), 
Llama 3.2 (3B), Gemma 2 (2B), and Mistral 7B — across three 
established clinical question answering benchmarks (MedQA, 
MedMCQA, and PubMedQA) using five controlled prompt styles 
designed to simulate realistic query reformulation. We 
introduce a quantitative consistency score that measures how 
often a model produces the same answer across prompt 
variations for the same question, and analyse this alongside 
overall accuracy, per-style accuracy, and instruction-following 
failure rates. All experiments were conducted locally on 
consumer CPU hardware using unmodified base models without 
medical fine-tuning, reflecting realistic deployment conditions 
in low-resource healthcare settings.

We report three central findings. First, consistency and 
accuracy are independent metrics: models that produce highly 
stable answers across prompt variations are not necessarily 
more correct, with Gemma 2 demonstrating the clearest case 
of high consistency alongside low accuracy. Second, roleplay 
prompt styles systematically reduce performance across all 
models and datasets, indicating that persona-based framing 
negatively affects clinical question answering reliability. 
Third, instruction-following failure rates vary independently 
of model size, demonstrating that larger parameter counts 
do not guarantee safer or more dependable outputs under 
constrained deployment conditions.

The remainder of this paper is structured as follows. 
Section~\ref{sec:related} reviews related work on clinical 
LLM evaluation, prompt sensitivity, and small language models 
in healthcare. Section~\ref{sec:methods} describes our 
datasets, models, prompt variation design, inference setup, 
and evaluation metrics. Section~\ref{sec:results} presents 
experimental results. Section~\ref{sec:discussion} discusses 
clinical implications, limitations, and future directions. 
Section~\ref{sec:conclusion} concludes the paper.

\section{Related Work}\label{sec:related}

\section{Methods}\label{sec:methods}

\subsection{Datasets}

We evaluated model consistency and accuracy using three established 
clinical question answering benchmarks. MedQA \citep{jin2021disease} 
contains multiple-choice questions derived from the United States Medical 
Licensing Examination (USMLE), representing clinical knowledge required 
for medical licensure. MedMCQA \citep{pal2022medmcqa} consists of 
multiple-choice questions from Indian medical entrance examinations 
(AIIMS/NEET), covering diverse specialties including pharmacology, 
anatomy, and pathology, with subject labels enabling specialty-level 
analysis. PubMedQA \citep{jin2019pubmedqa} presents biomedical research 
questions paired with PubMed abstracts, requiring yes, no, or maybe 
answers based on provided evidence rather than memorized knowledge. 
From each dataset we randomly sampled 200 questions using a fixed 
random seed (seed=42) to ensure reproducibility, yielding 600 
questions total across three distinct clinical reasoning contexts.

\subsection{Models}

We evaluated four open-source small language models spanning a 
parameter range of 2B to 7B to examine the effect of model scale 
on prompt consistency. The selected models were Phi-3 Mini 
\citep{abdin2024phi3} (3.8B parameters, Microsoft), Llama 3.2 
\citep{grattafiori2024llama} (3B parameters, Meta), Gemma 2 
\citep{team2024gemma} (2B parameters, Google), and Mistral 
\citep{jiang2023mistral} (7B parameters, Mistral AI). These models 
were selected because they are fully open-source, capable of running 
on consumer CPU hardware without GPU acceleration, and represent 
distinct model families — enabling cross-architecture comparison 
of consistency behavior. All inference was conducted locally using 
Ollama without any domain-specific fine-tuning, ensuring that 
observed consistency patterns reflect inherent model behavior 
rather than task-specific adaptation.

\subsection{Prompt Variation Design}

To systematically evaluate how small language models respond to 
differently worded clinical prompts, we designed a prompt variation 
engine that generates five semantically equivalent but stylistically 
distinct prompt formulations for each question.

\begin{itemize}
    \item \textbf{Original:} The question exactly as it appears in 
    the dataset, serving as the baseline condition.
    
    \item \textbf{Formal:} Rephrased in clinical academic language 
    to simulate how a healthcare professional might query a model 
    in a structured setting.
    
    \item \textbf{Simplified:} Written in plain everyday language, 
    simulating how a non-expert or patient might pose the same 
    question.
    
    \item \textbf{Roleplay:} The model is instructed to respond as 
    a practicing physician, testing whether persona-based prompting 
    affects answer consistency.
    
    \item \textbf{Direct:} Presents only the bare question and 
    answer options with minimal framing, assessing model behavior 
    without explicit instruction.
\end{itemize}

Each question across all three datasets was transformed into these 
five prompt styles, yielding 3,000 total prompts (600 questions 
$\times$ 5 styles). By keeping semantic content identical across 
variations, any differences in model output can be attributed to 
prompt sensitivity rather than question content.

\subsection{Inference Setup}

All models were served locally via Ollama and queried through its 
REST API. Inference was conducted with temperature set to 0 and a 
maximum token limit of 10 tokens per response. Setting temperature 
to 0 ensures fully deterministic outputs — the model produces 
identical responses for identical prompts — which is essential for 
isolating prompt sensitivity as the source of answer variation 
rather than stochastic sampling. Limiting output to 10 tokens 
prevents verbose responses and constrains the model to produce 
the single-letter (A, B, C, or D) or keyword (yes, no, or maybe) 
answer required by each dataset format.

Model responses were parsed using regular expression extraction. 
For multiple-choice datasets (MedQA and MedMCQA), the extractor 
identified the first standalone letter A, B, C, or D in the 
response. For PubMedQA, the extractor identified the first 
occurrence of yes, no, or maybe. Responses that did not contain 
a valid answer were categorized as \texttt{UNKNOWN}, representing 
instruction-following failures. All responses — including 
\texttt{UNKNOWN} outputs — were retained for consistency scoring 
and analysis.

\subsection{Evaluation Metrics}

We defined four complementary metrics to quantify model behavior 
under prompt variation.

\textbf{Consistency Score.} For each question, we identify the 
majority answer — the response selected most frequently across 
the five prompt styles — and compute the proportion of styles 
that agree with it. Formally, given responses 
$r_1, r_2, r_3, r_4, r_5$ for a question, the consistency 
score is:

\begin{equation}
    C = \frac{\sum_{i=1}^{5} \mathbf{1}[r_i = \hat{r}]}{5}
\end{equation}

where $\hat{r}$ is the majority answer and $\mathbf{1}[\cdot]$ 
is the indicator function. A score of 1.0 indicates perfect 
consistency across all prompt styles; a score of 0.2 indicates 
maximum inconsistency. For example, responses [A, B, B, B, A] 
yield a majority answer of B and a consistency score of 3/5 = 0.60.

\textbf{Overall Accuracy.} The majority answer for each question 
is compared against the dataset ground-truth label, reflecting 
task-level correctness while accounting for variability across 
prompt styles.

\textbf{Per-Style Accuracy.} Accuracy is computed separately 
for each prompt style (original, formal, simplified, roleplay, 
direct), enabling identification of which formulations the model 
handles most and least reliably.

\textbf{UNKNOWN Rate.} The proportion of responses that do not 
contain a valid answer option (A--D or yes/no/maybe), representing 
instruction-following failures that undermine clinical reliability.

Together these metrics provide a holistic assessment of model 
behavior: a model may be highly consistent but consistently wrong, 
accurate only under specific prompt styles, or prone to 
instruction-following failures. Evaluating any single metric 
in isolation would obscure these clinically relevant failure modes.

\section{Results}\label{sec:results}

\subsection{Overall Consistency and Accuracy}

Across all three datasets, consistency scores varied across models 
without a clear correlation with model size (Figure~\ref{fig:consistency}). 
Phi-3 Mini (3.8B) exhibited the lowest mean consistency scores 
(0.698--0.830), while Gemma 2 (2B) achieved the highest 
(0.845--0.888) despite being the smallest model tested. Mistral 7B 
demonstrated strong but slightly variable consistency (0.800--0.825), 
and Llama 3.2 showed intermediate consistency (0.774--0.807).

Accuracy patterns diverged substantially from consistency rankings 
(Figure~\ref{fig:accuracy}). Llama 3.2 achieved the highest overall 
accuracy across datasets (49.0--65.0\%), followed by Phi-3 Mini 
(48.0--53.0\%) and Mistral 7B (42.5--45.5\%). Gemma 2, despite 
exhibiting the highest consistency scores, achieved the lowest 
accuracy (33.0--43.5\%). This inverse relationship between 
consistency and accuracy in Gemma 2 highlights a critical finding: 
high consistency does not imply high accuracy. Models can be 
\textit{reliably wrong} — producing the same incorrect answer 
across all prompt variations — which represents a particularly 
dangerous failure mode in clinical decision support contexts.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig1_consistency_heatmap.png}
    \caption{Mean consistency scores across models and datasets. 
    Higher scores indicate greater agreement across prompt styles.}
    \label{fig:consistency}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig2_accuracy_heatmap.png}
    \caption{Overall accuracy across models and datasets, calculated 
    using majority answer against ground truth labels.}
    \label{fig:accuracy}
\end{figure}

\subsection{Effect of Prompt Style on Accuracy}

Prompt style demonstrated a consistent and measurable effect on 
model accuracy across all four models and three datasets 
(Figure~\ref{fig:style_accuracy}). The Roleplay prompt style 
— in which models were instructed to respond as a practicing 
physician — consistently underperformed relative to all other 
styles. The most pronounced decline was observed in Phi-3 Mini, 
which achieved 48.0\% accuracy under the Direct style but only 
26.5\% under Roleplay on MedQA, representing a drop of 21.5 
percentage points. Similar reductions were observed across 
Llama 3.2, Gemma 2, and Mistral 7B, confirming that roleplay 
prompting is systematically detrimental rather than 
model-specific (Figure~\ref{fig:roleplay_gap}).

In contrast, the Direct and Original prompt styles produced 
the highest and most stable accuracy across models and datasets, 
suggesting that minimal framing is preferable for reliable 
clinical question answering. These findings carry direct 
implications for healthcare AI deployment: prompt formulations 
intended to simulate clinical expertise — such as persona-based 
instructions — may paradoxically reduce the reliability of 
small language model outputs in patient-facing applications.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig4_accuracy_by_style.png}
    \caption{Accuracy by prompt style across models and datasets. 
    Roleplay consistently underperforms relative to other styles.}
    \label{fig:style_accuracy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig8_roleplay_gap.png}
    \caption{Roleplay prompt accuracy versus best performing 
    non-roleplay style across all models and datasets, with 
    annotated performance gaps.}
    \label{fig:roleplay_gap}
\end{figure}

\subsection{Instruction-Following Failure Rate}

The UNKNOWN rate — the proportion of responses that did not 
contain a valid answer option — varied substantially across 
models and datasets (Figure~\ref{fig:unknown}). Phi-3 Mini 
exhibited the highest UNKNOWN rate on MedQA at 10.5\%, 
indicating that approximately one in ten queries failed to 
produce a usable response. Mistral 7B, despite being the 
largest model evaluated, also demonstrated non-negligible 
UNKNOWN rates across all three datasets (4.7\%--7.2\%), 
suggesting that model size alone does not guarantee reliable 
instruction following.

In contrast, Llama 3.2 and Gemma 2 achieved the lowest UNKNOWN 
rates across datasets (0.8\%--2.8\% and 0.9\%--2.1\% 
respectively), consistently producing responses in the expected 
format. These results suggest that instruction adherence is a 
model-specific characteristic independent of parameter count.

From a clinical deployment perspective, high UNKNOWN rates 
represent a critical reliability concern. A model that 
frequently fails to produce a valid response cannot serve 
as a dependable decision support tool, as such failures 
interrupt clinical workflows, introduce uncertainty, and 
erode clinician trust in AI-assisted systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig5_unknown_rate.png}
    \caption{Instruction-following failure rates (UNKNOWN responses) 
    across models and datasets. Phi-3 Mini shows the highest failure 
    rate on MedQA at 10.5\%.}
    \label{fig:unknown}
\end{figure}

\subsection{Consistency Versus Accuracy Relationship}

Analysis of the relationship between mean consistency scores and 
overall accuracy revealed no clear positive correlation across 
models and datasets (Figure~\ref{fig:scatter}). Gemma 2 
exemplifies this disconnect most starkly: it achieved the highest 
consistency scores across all datasets (0.845--0.888) while 
simultaneously recording the lowest accuracy (33.0--43.5\%), 
indicating that it consistently produces incorrect answers with 
high confidence. Conversely, Llama 3.2 demonstrated moderate 
consistency (0.774--0.807) yet achieved the highest accuracy 
across most datasets (49.0--65.0\%), showing that a model can 
be more frequently correct even when its answers vary somewhat 
across prompt styles.

The distribution of consistency scores further illustrates 
model-level differences in reliability 
(Figure~\ref{fig:boxplot}). Phi-3 Mini exhibited the widest 
spread of consistency scores on MedQA, indicating high 
variability in prompt sensitivity across questions. Gemma 2 
showed a tighter, higher distribution — consistently stable 
but systematically inaccurate.

These findings demonstrate that consistency and accuracy are 
independent dimensions of model performance. Evaluating either 
metric in isolation is insufficient for clinical AI assessment. 
A model that is highly consistent but consistently wrong may 
be more dangerous than one that occasionally contradicts itself, 
as the former provides false confidence in systematically 
incorrect outputs. Safe clinical deployment therefore requires 
joint evaluation of both consistency and accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig3_consistency_vs_accuracy.png}
    \caption{Scatter plot of mean consistency score versus overall 
    accuracy across all models and datasets. No clear positive 
    correlation is observed, indicating that consistency and 
    accuracy are independent metrics.}
    \label{fig:scatter}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig7_consistency_distribution.png}
    \caption{Distribution of consistency scores across models and 
    datasets. Phi-3 Mini shows the widest spread on MedQA; 
    Gemma 2 shows a tight but high distribution reflecting 
    systematic consistency without accuracy.}
    \label{fig:boxplot}
\end{figure}

\section{Discussion}\label{sec:discussion}

\subsection{Consistency and Accuracy are Independent Metrics}

A central finding of this study is that consistency and accuracy 
represent independent dimensions of small language model performance 
in clinical question answering. While prior evaluations of clinical 
AI have predominantly focused on accuracy as the primary metric 
\citep{bedi2025systematic}, our results demonstrate that a model 
can achieve high consistency while remaining systematically 
inaccurate — a failure mode we term \textit{reliable incorrectness}.

Gemma 2 illustrates this most clearly, achieving the highest 
mean consistency scores across all datasets (0.845--0.888) while 
recording the lowest overall accuracy (33.0--43.5\%). This pattern 
suggests that the model has learned stable response tendencies that 
are nonetheless clinically wrong.

The danger of reliable incorrectness is as much psychological as 
technical. Consider a primary care physician using an AI assistant 
to support diagnostic reasoning. If the model returns the same 
answer regardless of how a question is phrased, this repetition 
creates an illusion of reliability. A clinician might reason: 
``it keeps suggesting pulmonary embolism — it must be confident.'' 
That apparent stability feels reassuring. But if the correct 
diagnosis is pneumonia, the model's consistency actively reinforces 
a false conclusion. Over time, clinicians may develop misplaced 
trust in systems that appear stable and decisive without recognising 
that the stability reflects systematic error rather than clinical 
knowledge. In practice, this could translate into repeated 
misdiagnoses, inappropriate investigations, or delayed treatment 
— not because the model behaves randomly, but because it is 
confidently and predictably incorrect.

This finding aligns with Kim et al. \citep{kim2025hallucination}, 
who demonstrate that foundation models frequently produce 
hallucinations with high apparent confidence, and that clinicians 
consider such outputs particularly dangerous precisely because 
they are difficult to detect. Our results extend this concern 
to the domain of prompt sensitivity: a model that produces 
consistent but wrong answers across varied prompt formulations 
compounds this risk by appearing robust to rephrasing.

These observations underscore that accuracy and consistency must 
be evaluated jointly in any clinical AI assessment framework. 
Neither metric alone is sufficient to characterise the safety 
profile of a model intended for healthcare deployment.

\subsection{Roleplay Prompts Consistently Underperform}

Across all four models and three datasets, roleplay-style prompts 
consistently produced lower accuracy than direct or original 
question formats. The most pronounced decline was observed in 
Phi-3 Mini on MedQA, where roleplay accuracy dropped 21.5 
percentage points below the best-performing style. Critically, 
this pattern was consistent across all model families tested, 
suggesting a systematic rather than model-specific phenomenon.

We propose that roleplay prompts introduce a form of task 
interference that is particularly detrimental for small language 
models. When a prompt begins with persona framing such as 
``You are a senior physician taking a licensing examination,'' 
the model must simultaneously interpret and simulate a clinical 
identity while reasoning through a structured medical question. 
For smaller models with limited representational capacity, 
this dual requirement may dilute attention from the core 
reasoning task. Furthermore, persona-based prompts may activate 
patterns learned from conversational or narrative training data 
rather than the structured exam-style reasoning required for 
clinical QA benchmarks — effectively shifting the task from 
question answering to performance-based text generation.

This finding has direct practical implications. Prompt engineering 
guidelines for clinical AI systems often recommend persona framing 
to make outputs sound more authoritative or clinically appropriate 
\citep{white2023prompt}. Our results suggest that for small 
open-source models, such framing may paradoxically reduce factual 
reliability. Developers building clinical decision support tools 
on small language models should favour minimal, direct prompt 
formulations over persona-based instructions.

This observation also extends findings from Ngweta et al. 
\citep{ngweta2024robustness}, who demonstrated that prompt 
format changes cause measurable performance variation in general 
NLP tasks. Our results show that in clinical settings, one 
specific format — roleplay — is systematically and substantially 
more harmful than others, representing a concrete and actionable 
finding for healthcare AI deployment.

\subsection{Implications for Low-Resource Healthcare Deployment}

A primary motivation for this study was the practical challenge 
of deploying AI in resource-constrained healthcare environments. 
Many rural clinics, community hospitals, and health systems in 
low-income settings lack access to GPU infrastructure, reliable 
high-speed internet, or cloud-based AI services. In these 
contexts, locally deployable open-source models in the 2B--7B 
parameter range represent a more realistic and equitable 
alternative to large proprietary systems. Our experimental 
design intentionally reflected these constraints: all models 
were evaluated on consumer CPU hardware without domain-specific 
fine-tuning, simulating realistic deployment conditions rather 
than idealised research environments.

Our findings provide concrete guidance for model selection in 
such settings. Although Gemma 2 demonstrated the highest 
consistency scores, its substantially lower accuracy suggests 
it may systematically mislead clinicians — a risk that is 
amplified in low-oversight environments where AI outputs may 
not be routinely verified. Mistral 7B, despite being the 
largest model evaluated, did not clearly outperform smaller 
alternatives and exhibited unexpected instruction-following 
failures, suggesting that larger parameter counts do not 
automatically justify greater hardware demands.

Llama 3.2 (3B) emerged as the most balanced candidate across 
our evaluation criteria, achieving the highest overall accuracy 
(49.0--65.0\%), moderate consistency, and among the lowest 
instruction-following failure rates. For low-resource healthcare 
deployment scenarios where clinician oversight may be limited, 
a model offering strong accuracy and reliable instruction 
adherence is preferable to one that maximises consistency 
at the expense of correctness.

More broadly, our results suggest that deployment decisions 
for clinical AI systems should be guided by joint evaluation 
of accuracy, consistency, and instruction compliance rather 
than any single metric. In settings where errors carry direct 
patient safety implications, the cost of systematic 
incorrectness — even when delivered consistently — outweighs 
the benefit of apparent stability. This study provides a 
practical evaluation framework for low-resource healthcare 
AI deployment that can be reproduced without specialised 
infrastructure, making it accessible to researchers and 
practitioners in exactly the settings it is designed to serve.

\subsection{Limitations}

Several limitations should be considered when interpreting 
these findings. First, we sampled 200 questions per dataset, 
which provides a computationally feasible and reproducible 
evaluation but may not fully capture the diversity of each 
benchmark. Second, the three datasets used — MedQA, MedMCQA, 
and PubMedQA — focus primarily on structured examination-style 
and research-based questions, which may not reflect the 
complexity of real-world clinical dialogue or unstructured 
patient interactions.

Third, we did not conduct human clinical evaluation to assess 
whether consistent but incorrect model outputs would 
meaningfully influence physician decision-making in practice. 
Such evaluation would strengthen the clinical validity of our 
findings but was beyond the scope of this study. Fourth, all 
models were evaluated without domain-specific fine-tuning, 
which may underestimate performance achievable through medical 
adaptation techniques such as instruction tuning or 
retrieval-augmented generation \citep{lewis2020rag}.

Fifth, our evaluation was restricted to multiple-choice and 
yes/no/maybe answer formats, whereas real clinical environments 
frequently involve open-ended reasoning, differential diagnosis, 
and contextual patient data. Sixth, hardware constraints 
limited our evaluation to models in the 2B--7B parameter range 
running on consumer CPU hardware, precluding direct comparison 
with larger proprietary systems such as GPT-4 or Claude. 
Finally, while our five prompt styles were designed to simulate 
realistic variation in clinical query formulation, they do not 
exhaust the full range of phrasings a clinician might use in 
practice.

Together, these limitations suggest that our results should 
be interpreted as an initial systematic analysis of prompt 
sensitivity under constrained deployment conditions rather 
than a comprehensive evaluation of all clinical AI scenarios. 
Future work should address these gaps through larger samples, 
broader dataset coverage, and human clinical validation studies.

\subsection{Future Work}

Several directions emerge naturally from this study. First, 
future research should evaluate whether domain-specific 
fine-tuning of small models on medical corpora reduces prompt 
sensitivity and improves consistency. If fine-tuned models 
demonstrate stronger stability under prompt variation, this 
would suggest that training data alignment — rather than 
model architecture — is the primary driver of inconsistency.

Second, integrating retrieval-augmented generation (RAG) 
pipelines with small clinical models represents a promising 
direction. RAG-augmented systems ground responses in 
retrieved evidence, which may reduce both factual 
hallucinations and prompt-induced answer variation. Testing 
whether external knowledge retrieval mitigates the 
consistency-accuracy tradeoff observed in this study would 
be a natural extension \citep{lewis2020rag}.

Third, evaluating larger open-source and proprietary models 
— when hardware resources permit — would clarify whether 
the independence between consistency and accuracy observed 
here persists at greater parameter scales, or whether it 
is specific to sub-7B models.

Beyond technical extensions, human-centered validation is 
essential. A controlled clinical study involving practicing 
physicians could assess how consistent but incorrect model 
outputs influence clinical reasoning, trust calibration, 
and decision-making in realistic workflows. Such a study 
would strengthen the ecological validity of the consistency 
metric introduced here.

Translationally, our findings support the development of 
a lightweight clinical decision support prototype for 
general practitioners in low-resource settings, built 
around the model demonstrating the strongest balance of 
accuracy and instruction adherence, with prompt engineering 
guardrails informed by our sensitivity analysis. Finally, 
extending this evaluation framework to multilingual datasets 
and non-English clinical benchmarks is critical, as many 
low-resource healthcare environments operate outside 
English-dominant settings. Together, these directions would 
advance this work from controlled benchmarking toward safe, 
deployable clinical AI systems grounded in both technical 
rigour and real-world constraints.

\section{Conclusion}\label{sec:conclusion}

In this study, we systematically evaluated prompt sensitivity 
in four small, CPU-runnable open-source language models 
(2B--7B parameters) across three clinical question answering 
datasets using five controlled prompt style variations, 
yielding 3,000 total evaluation instances.

We report three principal findings. First, consistency and 
accuracy are independent metrics: models that achieve high 
consistency are not necessarily more accurate, with Gemma 2 
demonstrating the most pronounced case of reliable 
incorrectness — achieving the highest consistency scores 
while recording the lowest accuracy across all datasets. 
Second, roleplay-style prompts consistently reduced accuracy 
across all models and datasets, suggesting that persona-based 
framing introduces systematic instability in clinical 
reasoning tasks and should be avoided in healthcare AI 
prompt design. Third, instruction-following failures varied 
substantially across models and were not determined by 
parameter count alone, indicating that model size is an 
insufficient proxy for deployment reliability under 
hardware constraints.

Taken together, these findings demonstrate that consistency 
alone cannot serve as a proxy for clinical reliability. 
Models that appear stable under prompt variation may still 
pose significant patient safety risks if their answers are 
systematically incorrect. Safe deployment of AI in clinical 
settings — particularly in low-resource environments where 
human oversight may be limited — requires joint evaluation 
of accuracy, consistency, and instruction adherence.

We call on the research community to move beyond 
accuracy-only evaluation frameworks for clinical AI and 
adopt multidimensional assessment approaches that reflect 
the reliability requirements of real-world healthcare 
deployment. The evaluation framework, datasets, and code 
introduced in this study are publicly available to support 
reproducibility and future research in this direction.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}

