\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{float}
\geometry{margin=1in}
\usepackage{longtable}
\usepackage{url}
\doublespacing

\title{\textbf{Prompt Sensitivity and Answer Consistency of Small
Open-Source Large Language Models on Clinical Question
Answering: Implications for Low-Resource Healthcare Deployment}}

\author{Shravani Hariprasad\\
\textit{Independent Researcher}}

\date{}

\begin{document}

\maketitle

\begin{abstract}

\textbf{Background:} Artificial intelligence is increasingly 
deployed in healthcare workflows, and small open-source language 
models are gaining attention as viable tools for low-resource 
settings where cloud infrastructure is unavailable. Despite 
their growing accessibility, the reliability of these models 
— particularly the stability of their outputs under different 
phrasings of the same clinical question — remains poorly 
understood.

\textbf{Objective:} This study systematically evaluates prompt 
sensitivity and answer consistency in small open-source language 
models on clinical question answering benchmarks, with 
implications for low-resource healthcare deployment.

\textbf{Methods:} We evaluated five open-source models — Gemma 2 (2B), Phi-3 Mini
(3.8B), Llama 3.2 (3B), Mistral 7B, and Meditron-7B
(domain-pretrained, not instruction-tuned) — across three clinical question answering datasets 
(MedQA, MedMCQA, and PubMedQA), sampling 200 questions from 
each. Each question was presented in five prompt styles 
(original, formal, simplified, roleplay, and direct). We 
measured a majority-based consistency score, overall and 
per-style accuracy, and instruction-following failure rate 
(UNKNOWN rate). All inference was conducted locally on 
consumer CPU hardware without domain-specific fine-tuning.

\textbf{Results:} Consistency and accuracy were largely 
independent across models and datasets. Gemma 2 achieved 
the highest consistency scores (0.845--0.888) but the 
lowest accuracy (33.0--43.5\%), while Llama 3.2 demonstrated 
moderate consistency (0.774--0.807) alongside the highest 
accuracy (49.0--65.0\%). Roleplay prompts consistently 
reduced accuracy across all models and datasets, with 
Phi-3 Mini showing the largest decline of 21.5 percentage 
points on MedQA. Instruction-following failure rates varied 
by model and were not determined by parameter count, with 
Phi-3 Mini exhibiting the highest UNKNOWN rate at 10.5\% 
on MedQA. Meditron-7B, a domain-pretrained model without 
instruction tuning, exhibited near-complete instruction-following 
failure on PubMedQA (99.0\% UNKNOWN rate), demonstrating 
that domain knowledge alone is insufficient for structured 
clinical question answering.

\textbf{Conclusions:} High consistency does not imply 
correctness in small clinical language models — models can 
be reliably wrong, representing a particularly dangerous 
failure mode in clinical decision support. Roleplay prompt 
styles should be avoided in healthcare AI applications. 
Among models evaluated, Llama 3.2 demonstrated the strongest 
balance of accuracy and reliability for low-resource 
deployment. These findings highlight the necessity of 
multidimensional evaluation frameworks that assess 
consistency, accuracy, and instruction adherence jointly 
for safe clinical AI deployment.

\end{abstract}

\noindent\textbf{Keywords:} prompt sensitivity, answer consistency, 
small language models, clinical question answering, large language 
models, low-resource healthcare, instruction following, medical AI

\newpage
\tableofcontents
\newpage

\section{Introduction}

Artificial intelligence is increasingly being integrated into 
healthcare workflows, from clinical decision support to 
exam-style medical question answering. While much attention 
has focused on large, cloud-based models, smaller open-source 
language models are gaining importance because they can run 
locally on standard CPUs, making them viable for rural clinics, 
community hospitals, and low-resource health systems with 
limited infrastructure \citep{garg2024slm}. However, as these 
smaller models become more accessible for real-world deployment, 
questions about their reliability become critical. In clinical 
settings, even small inaccuracies can have significant 
consequences, and the safety of AI systems depends not only 
on their overall performance but on how stable and predictable 
their outputs are under different conditions. Despite rapid 
adoption, the reliability of small, locally deployable models 
in healthcare remains insufficiently studied.

Most existing evaluations of clinical language models focus 
primarily on accuracy — whether a model selects the correct 
answer on a given benchmark \citep{bedi2025systematic}. 
However, accuracy alone does not capture how stable a model's 
outputs are when the same clinical question is phrased 
differently. In safety-critical domains such as healthcare, 
this distinction matters: a model that gives different answers 
depending on how a question is worded cannot be considered 
reliable, regardless of its aggregate accuracy score.

To our knowledge, no prior study has systematically examined 
whether small, CPU-runnable models provide consistent answers 
under controlled prompt variation in clinical settings. 
The CLEVER framework \citep{cleverjmir2025} evaluates clinical 
LLM outputs through physician preference but does not assess 
response stability across prompt reformulations. Ngweta et al. 
\citep{ngweta2024robustness} quantify prompt brittleness in 
general NLP tasks but do not examine clinical datasets or 
safety implications. Kim et al. \citep{kim2025hallucination} 
demonstrate that foundation models frequently produce confident 
but incorrect medical outputs, yet do not empirically measure 
variability under prompt changes. Recent surveys on small 
language models in healthcare \citep{garg2024slm} discuss 
efficiency and deployment tradeoffs without analysing answer 
consistency as a reliability dimension. This study addresses 
that gap directly.

In this work, we systematically evaluate prompt sensitivity in five open-source language 
models — Phi-3 Mini (3.8B), Llama 3.2 (3B), Gemma 2 (2B), Mistral 7B, 
and Meditron-7B (domain-pretrained) — across three 
established clinical question answering benchmarks (MedQA, 
MedMCQA, and PubMedQA) using five controlled prompt styles 
designed to simulate realistic query reformulation. We 
introduce a quantitative consistency score that measures how 
often a model produces the same answer across prompt 
variations for the same question, and analyse this alongside 
overall accuracy, per-style accuracy, and instruction-following 
failure rates. All experiments were conducted locally on 
consumer CPU hardware using unmodified base models without 
medical fine-tuning, reflecting realistic deployment conditions 
in low-resource healthcare settings.

We report four central findings. First, consistency and 
accuracy are independent metrics: models that produce highly 
stable answers across prompt variations are not necessarily 
more correct, with Gemma 2 demonstrating the clearest case 
of high consistency alongside low accuracy. Second, roleplay 
prompt styles systematically reduce performance across all 
models and datasets, indicating that persona-based framing 
negatively affects clinical question answering reliability. 
Third, instruction-following failure rates vary independently 
of model size, demonstrating that larger parameter counts 
do not guarantee safer or more dependable outputs under 
constrained deployment conditions.
Fourth, Meditron-7B, a domain-pretrained model without instruction 
tuning, exhibited near-complete instruction-following failure despite 
encoding substantial medical knowledge, demonstrating that domain 
knowledge and instruction-following capability are orthogonal 
requirements for clinical deployment.

The remainder of this paper is structured as follows. 
Section~\ref{sec:related} reviews related work on clinical 
LLM evaluation, prompt sensitivity, and small language models 
in healthcare. Section~\ref{sec:methods} describes our 
datasets, models, prompt variation design, inference setup, 
and evaluation metrics. Section~\ref{sec:results} presents 
experimental results. Section~\ref{sec:discussion} discusses 
clinical implications, limitations, and future directions. 
Section~\ref{sec:conclusion} concludes the paper.

\section{Related Work}\label{sec:related}

Most prior work evaluating large language models in healthcare 
has focused on accuracy, reasoning quality, and output safety. 
The CLEVER framework \citep{cleverjmir2025} evaluates clinical 
LLM outputs through blinded physician preference on tasks 
including summarisation and question answering, demonstrating 
that smaller domain-specific models can outperform larger 
general-purpose systems on clinically relevant criteria. 
Kim et al. \citep{kim2025hallucination} systematically examine 
medical hallucinations in foundation models, highlighting the 
dangers of confident but incorrect outputs and their potential 
to mislead clinical decision-making. While these studies 
provide critical insights into output quality and safety, 
neither investigates whether models produce consistent answers 
when the same clinical question is phrased differently — 
leaving prompt sensitivity largely unexplored in healthcare AI.

Growing interest in small, CPU-runnable language models has 
emerged in response to the practical constraints of healthcare 
deployment in low-resource settings. Garg et al. 
\citep{garg2024slm} survey small language models in healthcare, 
documenting their efficiency advantages and suitability for 
local deployment under limited infrastructure. The MedQA 
\citep{jin2021disease}, MedMCQA \citep{pal2022medmcqa}, and 
PubMedQA \citep{jin2019pubmedqa} benchmarks provide standardised 
evaluation tasks across diverse clinical question formats, and 
have been widely adopted for assessing compact models. These 
works collectively advance the case for small model deployment 
in accessible healthcare AI, but focus primarily on task 
accuracy and computational efficiency rather than answer 
stability under prompt variation.

Prompt sensitivity — the tendency of model outputs to change 
in response to semantically equivalent but stylistically 
different inputs — has been studied in general NLP research. 
Ngweta et al. \citep{ngweta2024robustness} demonstrate that 
minor changes in prompt formatting cause measurable performance 
fluctuations across multiple models and benchmarks, and propose 
mitigation strategies to reduce this brittleness. However, 
such investigations have not been extended to clinical question 
answering, and the safety implications of prompt sensitivity 
for healthcare AI deployment remain unexamined.

Taken together, these lines of research motivate the present 
study, which systematically evaluates prompt sensitivity and 
answer consistency in small open-source clinical language 
models — complementing prior work on accuracy, hallucination, 
and general prompt robustness with a focus on reliability 
under realistic deployment constraints.

\section{Methods}\label{sec:methods}

\subsection{Datasets}

We evaluated model consistency and accuracy using three established 
clinical question answering benchmarks. MedQA \citep{jin2021disease} 
contains multiple-choice questions derived from the United States Medical 
Licensing Examination (USMLE), representing clinical knowledge required 
for medical licensure. MedMCQA \citep{pal2022medmcqa} consists of multiple-choice 
questions from Indian medical entrance examinations (AIIMS/NEET), 
covering diverse medical specialties including pharmacology, 
anatomy, and pathology. Although the dataset includes subject 
labels, our evaluation used a randomly sampled subset of 200 
questions without stratification by specialty. PubMedQA \citep{jin2019pubmedqa} presents biomedical research 
questions paired with PubMed abstracts, requiring yes, no, or maybe 
answers based on provided evidence rather than memorized knowledge. 
From each dataset we randomly sampled 200 questions using a fixed 
random seed (seed=42) to ensure reproducibility, yielding 600 
questions total across three distinct clinical reasoning contexts.

\subsection{Models}

We evaluated five open-source small language models spanning a 
parameter range of 2B to 7B, including one domain-adapted medical pretraining model to examine the effect of model scale 
on prompt consistency. The selected models were Phi-3 Mini 
\citep{abdin2024phi3} (3.8B parameters, Microsoft), Llama 3.2 
\citep{grattafiori2024llama} (3B parameters, Meta), Gemma 2 
\citep{team2024gemma} (2B parameters, Google), and Mistral 
\citep{jiang2023mistral} (7B parameters, Mistral AI). These models 
were selected because they are fully open-source, capable of running 
on consumer CPU hardware without GPU acceleration, and represent 
distinct model families — enabling cross-architecture comparison 
of consistency behavior. All inference was conducted locally using 
Ollama without any domain-specific fine-tuning, ensuring that 
observed consistency patterns reflect inherent model behavior 
rather than task-specific adaptation.

In addition to these instruction-tuned general models, we 
included Meditron-7B \citep{chen2023meditron}, a domain-adapted 
language model developed by EPFL. Meditron-7B is built on 
Llama-2 and continued-pretrained on a curated medical corpus 
comprising PubMed articles, biomedical abstracts, and 
internationally recognised clinical practice guidelines. 
Critically, Meditron-7B is a base pretraining model without 
instruction tuning, meaning it has not been trained to follow 
structured task instructions. We include it explicitly to 
examine whether domain-specific medical pretraining alone — 
in the absence of instruction tuning — is sufficient for 
structured clinical question answering, and to contrast its 
behaviour with that of general instruction-tuned models. 
This comparison addresses a practical question for clinical 
AI deployment: is domain knowledge alone sufficient, or is 
instruction-following capability equally essential?

\subsection{Prompt Variation Design}

To systematically evaluate how small language models respond to 
differently worded clinical prompts, we designed a prompt variation 
engine that generates five semantically equivalent but stylistically 
distinct prompt formulations for each question.

\begin{itemize}
    \item \textbf{Original:} The question exactly as it appears in 
    the dataset, serving as the baseline condition.
    
    \item \textbf{Formal:} Rephrased in clinical academic language 
    to simulate how a healthcare professional might query a model 
    in a structured setting.
    
    \item \textbf{Simplified:} Written in plain everyday language, 
    simulating how a non-expert or patient might pose the same 
    question.
    
    \item \textbf{Roleplay:} The model is instructed to respond as 
    a practicing physician, testing whether persona-based prompting 
    affects answer consistency.
    
    \item \textbf{Direct:} Presents only the bare question and 
    answer options with minimal framing, assessing model behavior 
    without explicit instruction.
\end{itemize}

Each question across all three datasets was transformed into these 
five prompt styles, yielding 3,000 total prompts (600 questions 
$\times$ 5 styles). By keeping semantic content identical across 
variations, any differences in model output can be attributed to 
prompt sensitivity rather than question content.

\subsection{Inference Setup}

All models were served locally via Ollama and queried through its 
REST API. Inference was conducted with temperature set to 0 and a 
maximum token limit of 10 tokens per response. Setting temperature 
to 0 ensures fully deterministic outputs — the model produces 
identical responses for identical prompts — which is essential for 
isolating prompt sensitivity as the source of answer variation 
rather than stochastic sampling. Limiting output to 10 tokens 
prevents verbose responses and constrains the model to produce 
the single-letter (A, B, C, or D) or keyword (yes, no, or maybe) 
answer required by each dataset format.

Model responses were parsed using regular expression extraction. 
For multiple-choice datasets (MedQA and MedMCQA), the extractor 
identified the first standalone letter A, B, C, or D in the 
response. For PubMedQA, the extractor identified the first 
occurrence of yes, no, or maybe. Responses that did not contain 
a valid answer were categorized as \texttt{UNKNOWN}, representing 
instruction-following failures. All responses — including 
\texttt{UNKNOWN} outputs — were retained for consistency scoring 
and analysis.

\subsection{Evaluation Metrics}

We defined four complementary metrics to quantify model behavior 
under prompt variation.

\textbf{Consistency Score.} For each question, we identify the 
majority answer — the response selected most frequently across 
the five prompt styles — and compute the proportion of styles 
that agree with it. Formally, given responses 
$r_1, r_2, r_3, r_4, r_5$ for a question, the consistency 
score is:

\begin{equation}
    C = \frac{\sum_{i=1}^{5} \mathbf{1}[r_i = \hat{r}]}{5}
\end{equation}

where $\hat{r}$ is the majority answer and $\mathbf{1}[\cdot]$ 
is the indicator function. A score of 1.0 indicates perfect 
consistency across all prompt styles; a score of 0.2 indicates 
maximum inconsistency. For example, responses [A, B, B, B, A] 
yield a majority answer of B and a consistency score of 3/5 = 0.60.

\textbf{Overall Accuracy.} The majority answer for each question 
is compared against the dataset ground-truth label, reflecting 
task-level correctness while accounting for variability across 
prompt styles.

\textbf{Per-Style Accuracy.} Accuracy is computed separately 
for each prompt style (original, formal, simplified, roleplay, 
direct), enabling identification of which formulations the model 
handles most and least reliably.

\textbf{UNKNOWN Rate.} The proportion of responses that do not 
contain a valid answer option (A--D or yes/no/maybe), representing 
instruction-following failures that undermine clinical reliability.

Together these metrics provide a holistic assessment of model 
behavior: a model may be highly consistent but consistently wrong, 
accurate only under specific prompt styles, or prone to 
instruction-following failures. Evaluating any single metric 
in isolation would obscure these clinically relevant failure modes.

% \clearpage
\section{Results}\label{sec:results}

\subsection{Overall Consistency and Accuracy}

\begin{table}[H]
\centering
\small
\caption{Summary of consistency scores, overall accuracy, and 
instruction-following failure rates across all models and datasets. 
Best value among instruction-tuned models shown in \textbf{bold}. 
$\dagger$Meditron-7B is a pretraining model without instruction 
tuning — high UNKNOWN rates reflect instruction-following failure 
rather than lack of domain knowledge.}
\label{tab:results}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Mean} & \textbf{Fully} & \textbf{Overall} & \textbf{UNKNOWN} & \textbf{Acc.} & \textbf{Acc.} & \textbf{Acc.} & \textbf{Acc.} & \textbf{Acc.} \\
 & & \textbf{Cons.} & \textbf{Cons.} & \textbf{Acc.} & \textbf{Rate} & \textbf{Orig.} & \textbf{Form.} & \textbf{Simp.} & \textbf{Role.} & \textbf{Dir.} \\
 & & & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} & \textbf{(\%)} \\
\midrule
\multirow{5}{*}{MedQA}
 & Phi-3 Mini  & 0.698 & 19.5 & 48.0 & 10.5 & 44.5 & 44.5 & 42.5 & 26.5 & 48.0 \\
 & Llama 3.2   & 0.776 & 35.0 & \textbf{49.0} & \textbf{0.8} & 47.5 & 44.5 & 46.0 & 47.0 & 43.5 \\
 & Gemma 2     & \textbf{0.888} & \textbf{63.5} & 40.0 & 2.1 & 39.0 & 39.0 & 37.0 & 35.0 & 35.0 \\
 & Mistral 7B  & 0.800 & 41.0 & 45.0 & 4.7 & 45.5 & 41.5 & 43.0 & 34.0 & \textbf{50.5} \\
 & Meditron$\dagger$ & 0.761 & 10.0 & 30.0 & 22.8 & 4.0 & 26.5 & 28.0 & 29.5 & 30.5 \\
\midrule
\multirow{5}{*}{MedMCQA}
 & Phi-3 Mini  & 0.730 & 27.0 & 53.0 & 4.6 & 52.0 & 49.5 & 48.5 & 37.5 & 52.0 \\
 & Llama 3.2   & 0.774 & 38.5 & \textbf{55.5} & \textbf{1.5} & \textbf{56.5} & 50.5 & 51.5 & 48.0 & 53.0 \\
 & Gemma 2     & \textbf{0.851} & \textbf{56.5} & 43.5 & 0.9 & 42.5 & 39.0 & 38.0 & 40.5 & 44.0 \\
 & Mistral 7B  & 0.812 & 45.5 & 45.5 & 4.7 & 44.5 & 43.0 & 42.5 & 41.5 & 44.0 \\
 & Meditron$\dagger$ & 0.762 & 8.0 & 33.5 & 22.4 & 4.0 & 33.0 & 30.0 & 33.5 & 29.5 \\
\midrule
\multirow{5}{*}{PubMedQA}
 & Phi-3 Mini  & 0.830 & 41.0 & 48.0 & 0.0 & 42.5 & \textbf{53.5} & 52.5 & 47.5 & 31.0 \\
 & Llama 3.2   & 0.807 & 45.5 & \textbf{65.0} & 2.8 & \textbf{59.5} & 51.0 & \textbf{64.5} & \textbf{65.0} & 56.5 \\
 & Gemma 2     & \textbf{0.845} & \textbf{50.5} & 33.0 & \textbf{1.3} & 28.0 & 41.5 & 47.5 & 27.0 & 30.5 \\
 & Mistral 7B  & 0.825 & 49.5 & 42.5 & 7.2 & 39.5 & 39.5 & 45.5 & 34.5 & 45.5 \\
 & Meditron$\dagger$ & 0.010 & 0.0 & 2.0 & 99.0 & 0.0 & 0.0 & 1.5 & 0.0 & 0.5 \\
\bottomrule
\end{tabular}
}
\begin{tablenotes}
\small
\item $\dagger$ Meditron 7B not instruction-tuned. High UNKNOWN rates reflect 
instruction-following failure, not lack of domain knowledge.
\end{tablenotes}
\end{table}

Statistical significance of observed differences was assessed 
using the Wilcoxon signed-rank test for consistency scores 
and McNemar's test for accuracy comparisons. Consistency 
differences between models were statistically significant 
on MedQA and MedMCQA (p$<$0.001 for most pairwise 
comparisons) but not on PubMedQA, suggesting that dataset 
characteristics influence the degree of prompt sensitivity. 
Accuracy differences were generally non-significant on MedQA, 
while on PubMedQA, Llama 3.2 significantly outperformed both 
Phi-3 Mini and Gemma 2 (p$<$0.001), confirming its superior 
clinical reasoning on biomedical evidence-based questions. 
Full significance test results are provided in 
Table~\ref{tab:stats}.

\begin{longtable}{llllcc}
\caption{Pairwise statistical significance tests. Wilcoxon 
signed-rank test for consistency; McNemar test for accuracy.
*** p$<$0.001, ** p$<$0.01, * p$<$0.05, ns = not significant.
$\dagger$Meditron PubMedQA comparisons reflect instruction-following 
failure (99\% UNKNOWN rate).}
\label{tab:stats}\\
\toprule
\textbf{Dataset} & \textbf{M1} & \textbf{M2} & 
\textbf{Metric} & \textbf{p} & \textbf{Sig.} \\
\midrule
\endfirsthead
\toprule
\textbf{Dataset} & \textbf{M1} & \textbf{M2} & 
\textbf{Metric} & \textbf{p} & \textbf{Sig.} \\
\midrule
\endhead
\midrule
\multicolumn{6}{r}{\small\itshape Continued on next page} \\
\endfoot
\bottomrule
\endlastfoot
MedQA & Llama 3.2 & Phi-3 & Cons. & 0.0005 & *** \\
 & Gemma 2 & Phi-3 & Cons. & $<$.0001 & *** \\
 & Mistral & Phi-3 & Cons. & $<$.0001 & *** \\
 & Llama 3.2 & Gemma 2 & Cons. & $<$.0001 & *** \\
 & Meditron & Phi-3 & Cons. & $<$.0001 & *** \\
 & Meditron & Llama 3.2 & Cons. & 0.3342 & ns \\
 & Llama 3.2 & Phi-3 & Acc. & 0.9020 & ns \\
 & Gemma 2 & Phi-3 & Acc. & 0.0689 & ns \\
 & Mistral & Phi-3 & Acc. & 0.5383 & ns \\
 & Llama 3.2 & Gemma 2 & Acc. & 0.0512 & ns \\
 & Meditron & Phi-3 & Acc. & 0.0004 & *** \\
 & Meditron & Llama 3.2 & Acc. & 0.0001 & *** \\
\midrule
MedMCQA & Llama 3.2 & Phi-3 & Cons. & 0.0437 & * \\
 & Gemma 2 & Phi-3 & Cons. & $<$.0001 & *** \\
 & Mistral & Phi-3 & Cons. & 0.0001 & *** \\
 & Llama 3.2 & Gemma 2 & Cons. & 0.0001 & *** \\
 & Meditron & Phi-3 & Cons. & 0.0007 & *** \\
 & Meditron & Llama 3.2 & Cons. & 0.2052 & ns \\
 & Llama 3.2 & Phi-3 & Acc. & 0.5962 & ns \\
 & Gemma 2 & Phi-3 & Acc. & 0.0327 & * \\
 & Mistral & Phi-3 & Acc. & 0.1060 & ns \\
 & Llama 3.2 & Gemma 2 & Acc. & 0.0075 & ** \\
 & Meditron & Phi-3 & Acc. & 0.0002 & *** \\
 & Meditron & Llama 3.2 & Acc. & $<$.0001 & *** \\
\midrule
PubMedQA & Llama 3.2 & Phi-3 & Cons. & 0.1055 & ns \\
 & Gemma 2 & Phi-3 & Cons. & 0.8497 & ns \\
 & Mistral & Phi-3 & Cons. & 0.6172 & ns \\
 & Llama 3.2 & Gemma 2 & Cons. & 0.1055 & ns \\
 & Meditron$\dagger$ & Phi-3 & Cons. & $<$.0001 & *** \\
 & Meditron$\dagger$ & Llama 3.2 & Cons. & $<$.0001 & *** \\
 & Llama 3.2 & Phi-3 & Acc. & $<$.0001 & *** \\
 & Gemma 2 & Phi-3 & Acc. & 0.0002 & *** \\
 & Mistral & Phi-3 & Acc. & 0.1930 & ns \\
 & Llama 3.2 & Gemma 2 & Acc. & $<$.0001 & *** \\
 & Meditron & Phi-3 & Acc. & $<$.0001 & *** \\
 & Meditron & Llama 3.2 & Acc. & $<$.0001 & *** \\
\end{longtable}

Across all three datasets, consistency scores varied across models 
without a clear correlation with model size (Figure~\ref{fig:consistency}). 
Phi-3 Mini (3.8B) exhibited the lowest mean consistency scores 
(0.698--0.830), while Gemma 2 (2B) achieved the highest 
(0.845--0.888) despite being the smallest model tested. Mistral 7B 
demonstrated strong but slightly variable consistency (0.800--0.825), 
and Llama 3.2 showed intermediate consistency (0.774--0.807).

Accuracy patterns diverged substantially from consistency rankings 
(Figure~\ref{fig:accuracy}). Llama 3.2 achieved the highest overall 
accuracy across datasets (49.0--65.0\%), followed by Phi-3 Mini 
(48.0--53.0\%) and Mistral 7B (42.5--45.5\%). Gemma 2, despite 
exhibiting the highest consistency scores, achieved the lowest 
accuracy (33.0--43.5\%). This inverse relationship between 
consistency and accuracy in Gemma 2 highlights a critical finding: 
high consistency does not imply high accuracy. Models can be 
\textit{reliably wrong} — producing the same incorrect answer 
across all prompt variations — which represents a particularly 
dangerous failure mode in clinical decision support contexts.

Meditron-7B, included as a domain-adapted pretraining model 
without instruction tuning, exhibited markedly different 
behaviour from all instruction-tuned models. While it 
achieved moderate consistency on MedQA and MedMCQA 
(0.761 and 0.762 respectively), its UNKNOWN rates were 
substantially higher (22.8\% and 22.4\%), indicating 
frequent instruction-following failures. On PubMedQA, 
Meditron-7B produced near-complete failure — a consistency 
score of 0.010, 2.0\% accuracy, and 99.0\% UNKNOWN rate 
— suggesting the model is unable to produce structured 
yes/no/maybe responses without instruction tuning. These 
results indicate that domain-specific pretraining alone 
is insufficient for structured clinical question answering, 
and that instruction-following capability is a prerequisite 
for clinical deployment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig1_consistency_heatmap.png}
    \caption{Mean consistency scores across models and datasets. 
    Higher scores indicate greater agreement across prompt styles.}
    \label{fig:consistency}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig2_accuracy_heatmap.png}
    \caption{Overall accuracy across models and datasets, calculated 
    using majority answer against ground truth labels.}
    \label{fig:accuracy}
\end{figure}

\subsection{Effect of Prompt Style on Accuracy}

Prompt style demonstrated a consistent and measurable effect on 
model accuracy across all five models and three datasets 
(Figure~\ref{fig:style_accuracy}). The Roleplay prompt style 
— in which models were instructed to respond as a practicing 
physician — consistently underperformed relative to all other 
styles. The most pronounced decline was observed in Phi-3 Mini, 
which achieved 48.0\% accuracy under the Direct style but only 
26.5\% under Roleplay on MedQA, representing a drop of 21.5 
percentage points. Similar reductions were observed across 
Llama 3.2, Gemma 2, and Mistral 7B, confirming that roleplay 
prompting is systematically detrimental rather than 
model-specific (Figure~\ref{fig:roleplay_gap}).

In contrast, the Direct and Original prompt styles produced 
the highest and most stable accuracy across models and datasets, 
suggesting that minimal framing is preferable for reliable 
clinical question answering. These findings carry direct 
implications for healthcare AI deployment: prompt formulations 
intended to simulate clinical expertise — such as persona-based 
instructions — may paradoxically reduce the reliability of 
small language model outputs in patient-facing applications.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig3_accuracy_by_style.png}
    \caption{Accuracy by prompt style across models and datasets. 
    Roleplay consistently underperforms relative to other styles.}
    \label{fig:style_accuracy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig4_roleplay_gap.png}
    \caption{Roleplay prompt accuracy versus best performing 
    non-roleplay style across all models and datasets, with 
    annotated performance gaps.}
    \label{fig:roleplay_gap}
\end{figure}

\subsection{Instruction-Following Failure Rate}

The UNKNOWN rate — the proportion of responses that did not 
contain a valid answer option — varied substantially across 
models and datasets (Figure~\ref{fig:unknown}). Phi-3 Mini 
exhibited the highest UNKNOWN rate on MedQA at 10.5\%, 
indicating that approximately one in ten queries failed to 
produce a usable response. Mistral 7B, despite being the 
largest model evaluated, also demonstrated non-negligible 
UNKNOWN rates across all three datasets (4.7\%--7.2\%), 
suggesting that model size alone does not guarantee reliable 
instruction following.

In contrast, Llama 3.2 and Gemma 2 achieved the lowest UNKNOWN 
rates across datasets (0.8\%--2.8\% and 0.9\%--2.1\% 
respectively), consistently producing responses in the expected 
format. These results suggest that instruction adherence is a 
model-specific characteristic independent of parameter count.

Meditron-7B exhibited the most severe instruction-following 
failures of all models evaluated, with UNKNOWN rates of 
22.8\% on MedQA, 22.4\% on MedMCQA, and 99.0\% on 
PubMedQA. The near-complete failure on PubMedQA — where 
99 of 100 responses were invalid — is consistent with 
Meditron's architecture as a base pretraining model 
without instruction tuning. Unlike the instruction-following 
failures observed in Phi-3 Mini, which appear to be 
prompt-style dependent, Meditron's failures reflect a 
fundamental absence of structured response capability 
rather than prompt sensitivity.

From a clinical deployment perspective, high UNKNOWN rates 
represent a critical reliability concern. A model that 
frequently fails to produce a valid response cannot serve 
as a dependable decision support tool, as such failures 
interrupt clinical workflows, introduce uncertainty, and 
erode clinician trust in AI-assisted systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig5_unknown_rate.png}
    \caption{Instruction-following failure rates (UNKNOWN responses) 
    across models and datasets. Phi-3 Mini shows the highest failure 
    rate on MedQA at 10.5\%.}
    \label{fig:unknown}
\end{figure}

\subsection{Consistency Versus Accuracy Relationship}

Analysis of the relationship between mean consistency scores and 
overall accuracy revealed no clear positive correlation across 
models and datasets (Figure~\ref{fig:scatter}). Gemma 2 
exemplifies this disconnect most starkly: it achieved the highest 
consistency scores across all datasets (0.845--0.888) while 
simultaneously recording the lowest accuracy (33.0--43.5\%), 
indicating that it consistently produces incorrect answers with 
high confidence. Conversely, Llama 3.2 demonstrated moderate 
consistency (0.774--0.807) yet achieved the highest accuracy 
across most datasets (49.0--65.0\%), showing that a model can 
be more frequently correct even when its answers vary somewhat 
across prompt styles.

The distribution of consistency scores further illustrates 
model-level differences in reliability 
(Figure~\ref{fig:boxplot}). Phi-3 Mini exhibited the widest 
spread of consistency scores on MedQA, indicating high 
variability in prompt sensitivity across questions. Gemma 2 
showed a tighter, higher distribution — consistently stable 
but systematically inaccurate.

These findings demonstrate that consistency and accuracy are 
independent dimensions of model performance. Evaluating either 
metric in isolation is insufficient for clinical AI assessment. 
A model that is highly consistent but consistently wrong may 
be more dangerous than one that occasionally contradicts itself, 
as the former provides false confidence in systematically 
incorrect outputs. Safe clinical deployment therefore requires 
joint evaluation of both consistency and accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig6_consistency_vs_accuracy.png}
    \caption{Scatter plot of mean consistency score versus overall 
    accuracy across all models and datasets. No clear positive 
    correlation is observed, indicating that consistency and 
    accuracy are independent metrics.}
    \label{fig:scatter}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/fig7_consistency_distribution.png}
    \caption{Distribution of consistency scores across models and 
    datasets. Phi-3 Mini shows the widest spread on MedQA; 
    Gemma 2 shows a tight but high distribution reflecting 
    systematic consistency without accuracy.}
    \label{fig:boxplot}
\end{figure}

\section{Discussion}\label{sec:discussion}

\subsection{Consistency and Accuracy are Independent Metrics}

A central finding of this study is that consistency and accuracy 
represent independent dimensions of small language model performance 
in clinical question answering. While prior evaluations of clinical 
AI have predominantly focused on accuracy as the primary metric 
\citep{bedi2025systematic}, our results demonstrate that a model 
can achieve high consistency while remaining systematically 
inaccurate — a failure mode we term \textit{reliable incorrectness}.

Gemma 2 illustrates this most clearly, achieving the highest 
mean consistency scores across all datasets (0.845--0.888) while 
recording the lowest overall accuracy (33.0--43.5\%). This pattern 
suggests that the model has learned stable response tendencies that 
are nonetheless clinically wrong.

The danger of reliable incorrectness is as much psychological as 
technical. Consider a primary care physician using an AI assistant 
to support diagnostic reasoning. If the model returns the same 
answer regardless of how a question is phrased, this repetition 
creates an illusion of reliability. A clinician might reason: 
``it keeps suggesting pulmonary embolism — it must be confident.'' 
That apparent stability feels reassuring. But if the correct 
diagnosis is pneumonia, the model's consistency actively reinforces 
a false conclusion. Over time, clinicians may develop misplaced 
trust in systems that appear stable and decisive without recognising 
that the stability reflects systematic error rather than clinical 
knowledge. In practice, this could translate into repeated 
misdiagnoses, inappropriate investigations, or delayed treatment 
— not because the model behaves randomly, but because it is 
confidently and predictably incorrect.

This finding aligns with Kim et al. \citep{kim2025hallucination}, 
who demonstrate that foundation models frequently produce 
hallucinations with high apparent confidence, and that clinicians 
consider such outputs particularly dangerous precisely because 
they are difficult to detect. Our results extend this concern 
to the domain of prompt sensitivity: a model that produces 
consistent but wrong answers across varied prompt formulations 
compounds this risk by appearing robust to rephrasing.

These observations underscore that accuracy and consistency must 
be evaluated jointly in any clinical AI assessment framework. 
Neither metric alone is sufficient to characterise the safety 
profile of a model intended for healthcare deployment.

\subsection{Roleplay Prompts Consistently Underperform}

Across all five models and three datasets, roleplay-style prompts 
consistently produced lower accuracy than direct or original 
question formats. The most pronounced decline was observed in 
Phi-3 Mini on MedQA, where roleplay accuracy dropped 21.5 
percentage points below the best-performing style. Critically, 
this pattern was consistent across all model families tested, 
suggesting a systematic rather than model-specific phenomenon.

We propose that roleplay prompts introduce a form of task 
interference that is particularly detrimental for small language 
models. When a prompt begins with persona framing such as 
``You are a senior physician taking a licensing examination,'' 
the model must simultaneously interpret and simulate a clinical 
identity while reasoning through a structured medical question. 
For smaller models with limited representational capacity, 
this dual requirement may dilute attention from the core 
reasoning task. Furthermore, persona-based prompts may activate 
patterns learned from conversational or narrative training data 
rather than the structured exam-style reasoning required for 
clinical QA benchmarks — effectively shifting the task from 
question answering to performance-based text generation.

This finding has direct practical implications. Prompt engineering 
guidelines for clinical AI systems often recommend persona framing 
to make outputs sound more authoritative or clinically appropriate 
\citep{white2023prompt}. Our results suggest that for small 
open-source models, such framing may paradoxically reduce factual 
reliability. Developers building clinical decision support tools 
on small language models should favour minimal, direct prompt 
formulations over persona-based instructions.

This observation also extends findings from Ngweta et al. 
\citep{ngweta2024robustness}, who demonstrated that prompt 
format changes cause measurable performance variation in general 
NLP tasks. Our results show that in clinical settings, one 
specific format — roleplay — is systematically and substantially 
more harmful than others, representing a concrete and actionable 
finding for healthcare AI deployment.

\subsection{Implications for Low-Resource Healthcare Deployment}

A primary motivation for this study was the practical challenge 
of deploying AI in resource-constrained healthcare environments. 
Many rural clinics, community hospitals, and health systems in 
low-income settings lack access to GPU infrastructure, reliable 
high-speed internet, or cloud-based AI services. In these 
contexts, locally deployable open-source models in the 2B--7B 
parameter range represent a more realistic and equitable 
alternative to large proprietary systems. Our experimental 
design intentionally reflected these constraints: all models 
were evaluated on consumer CPU hardware without domain-specific 
fine-tuning, simulating realistic deployment conditions rather 
than idealised research environments.

Our findings provide concrete guidance for model selection in 
such settings. Although Gemma 2 demonstrated the highest 
consistency scores, its substantially lower accuracy suggests 
it may systematically mislead clinicians — a risk that is 
amplified in low-oversight environments where AI outputs may 
not be routinely verified. Mistral 7B, despite being the 
largest model evaluated, did not clearly outperform smaller 
alternatives and exhibited unexpected instruction-following 
failures, suggesting that larger parameter counts do not 
automatically justify greater hardware demands.

Llama 3.2 (3B) emerged as the most balanced candidate across 
our evaluation criteria, achieving the highest overall accuracy 
(49.0--65.0\%), moderate consistency, and among the lowest 
instruction-following failure rates. For low-resource healthcare 
deployment scenarios where clinician oversight may be limited, 
a model offering strong accuracy and reliable instruction 
adherence is preferable to one that maximises consistency 
at the expense of correctness.

More broadly, our results suggest that deployment decisions 
for clinical AI systems should be guided by joint evaluation 
of accuracy, consistency, and instruction compliance rather 
than any single metric. In settings where errors carry direct 
patient safety implications, the cost of systematic 
incorrectness — even when delivered consistently — outweighs 
the benefit of apparent stability. This study provides a 
practical evaluation framework for low-resource healthcare 
AI deployment that can be reproduced without specialised 
infrastructure, making it accessible to researchers and 
practitioners in exactly the settings it is designed to serve.

\subsection{Domain Knowledge Versus Instruction Following}

The inclusion of Meditron-7B reveals an important distinction 
between two capabilities required for clinical AI deployment: 
domain knowledge and instruction adherence. Meditron-7B was 
pretrained on a curated medical corpus comprising PubMed 
articles and clinical guidelines \citep{chen2023meditron}, 
encoding substantial medical domain knowledge. However, 
without instruction tuning, it failed to produce structured 
responses in the required format across all three datasets, 
with UNKNOWN rates reaching 99.0\% on PubMedQA.

This finding suggests that domain knowledge and task 
usability are orthogonal capabilities. A model can encode 
rich medical knowledge through continued pretraining yet 
remain clinically unusable for structured question answering 
without the ability to follow task-specific instructions. 
For practitioners deploying AI in clinical settings, this 
distinction has direct practical implications: selecting 
a model based on its training data alone — without 
verifying instruction-following capability — may result 
in a system that is knowledgeable but functionally 
unusable in structured workflows.

This also motivates a clear direction for future work: 
instruction-tuning Meditron or similar domain-pretrained 
models on structured clinical QA data, and evaluating 
whether the combination of domain knowledge and 
instruction following produces models that are both 
more accurate and more consistent than general 
instruction-tuned alternatives.

\subsection{Limitations}
Several limitations should be considered when interpreting 
these findings. First, we sampled 200 questions per dataset, 
which provides a computationally feasible and reproducible 
evaluation but may not fully capture the diversity of each 
benchmark. Second, the three datasets used — MedQA, MedMCQA, 
and PubMedQA — focus primarily on structured examination-style 
and research-based questions, which may not reflect the 
complexity of real-world clinical dialogue or unstructured 
patient interactions.

Third, we did not conduct human clinical evaluation to assess 
whether consistent but incorrect model outputs would 
meaningfully influence physician decision-making in practice. 
Such evaluation would strengthen the clinical validity of our 
findings but was beyond the scope of this study. Fourth, all 
models were evaluated without domain-specific fine-tuning, 
which may underestimate performance achievable through medical 
adaptation techniques such as instruction tuning or 
retrieval-augmented generation \citep{lewis2020rag}.

Fifth, our evaluation was restricted to multiple-choice and 
yes/no/maybe answer formats, whereas real clinical environments 
frequently involve open-ended reasoning, differential diagnosis, 
and contextual patient data. Sixth, hardware constraints 
limited our evaluation to models in the 2B--7B parameter range 
running on consumer CPU hardware, precluding direct comparison 
with larger proprietary systems such as GPT-4 or Claude. 
Seventh, while our five prompt styles were designed to simulate 
realistic variation in clinical query formulation, they do not 
exhaust the full range of phrasings a clinician might use in 
practice. Finally, Meditron-7B was evaluated in its base 
pretraining form without instruction tuning, which limits 
direct comparison with instruction-tuned models and may 
underrepresent the potential of domain-adapted models when 
properly instruction-tuned for structured clinical tasks.

Together, these limitations suggest that our results should 
be interpreted as an initial systematic analysis of prompt 
sensitivity under constrained deployment conditions rather 
than a comprehensive evaluation of all clinical AI scenarios. 
Future work should address these gaps through larger samples, 
broader dataset coverage, and human clinical validation studies.
\subsection{Future Work}

Several directions emerge naturally from this study. First, 
future research should evaluate whether domain-specific 
fine-tuning of small models on medical corpora reduces prompt 
sensitivity and improves consistency. If fine-tuned models 
demonstrate stronger stability under prompt variation, this 
would suggest that training data alignment — rather than 
model architecture — is the primary driver of inconsistency.

In particular, instruction-tuning Meditron-7B or similar 
domain-pretrained models on structured clinical QA data 
represents a direct next step — testing whether combining 
domain knowledge with instruction-following capability 
produces models that are both more accurate and more 
consistent than general instruction-tuned alternatives 
evaluated in this study.

Second, integrating retrieval-augmented generation (RAG) 
pipelines with small clinical models represents a promising 
direction. RAG-augmented systems ground responses in 
retrieved evidence, which may reduce both factual 
hallucinations and prompt-induced answer variation. Testing 
whether external knowledge retrieval mitigates the 
consistency-accuracy tradeoff observed in this study would 
be a natural extension \citep{lewis2020rag}.

Third, evaluating larger open-source and proprietary models 
— when hardware resources permit — would clarify whether 
the independence between consistency and accuracy observed 
here persists at greater parameter scales, or whether it 
is specific to sub-7B models.

Beyond technical extensions, human-centered validation is 
essential. A controlled clinical study involving practicing 
physicians could assess how consistent but incorrect model 
outputs influence clinical reasoning, trust calibration, 
and decision-making in realistic workflows. Such a study 
would strengthen the ecological validity of the consistency 
metric introduced here.

Translationally, our findings support the development of 
a lightweight clinical decision support prototype for 
general practitioners in low-resource settings, built 
around the model demonstrating the strongest balance of 
accuracy and instruction adherence, with prompt engineering 
guardrails informed by our sensitivity analysis. Finally, 
extending this evaluation framework to multilingual datasets 
and non-English clinical benchmarks is critical, as many 
low-resource healthcare environments operate outside 
English-dominant settings. Together, these directions would 
advance this work from controlled benchmarking toward safe, 
deployable clinical AI systems grounded in both technical 
rigour and real-world constraints.

\section{Conclusion}\label{sec:conclusion}

In this study, we systematically evaluated prompt sensitivity 
in five open-source language models (2B--7B parameters), 
including four general instruction-tuned models and one 
domain-adapted pretraining model, across three clinical 
question answering datasets using five controlled prompt 
style variations, yielding 3,000 total evaluation instances.

We report four principal findings. First, consistency and 
accuracy are independent metrics: models that achieve high 
consistency are not necessarily more accurate, with Gemma 2 
demonstrating the most pronounced case of reliable 
incorrectness — achieving the highest consistency scores 
while recording the lowest accuracy across all datasets. 
Second, roleplay-style prompts consistently reduced accuracy 
across all models and datasets, suggesting that persona-based 
framing introduces systematic instability in clinical 
reasoning tasks and should be avoided in healthcare AI 
prompt design. Third, instruction-following failures varied 
substantially across models and were not determined by 
parameter count alone, indicating that model size is an 
insufficient proxy for deployment reliability under 
hardware constraints. Fourth, Meditron-7B, a domain-adapted model without 
instruction tuning, exhibited near-complete failure on 
structured clinical QA tasks despite encoding substantial 
medical domain knowledge — demonstrating that domain 
knowledge and instruction-following capability are 
orthogonal requirements for clinical AI deployment.

Taken together, these findings demonstrate that consistency 
alone cannot serve as a proxy for clinical reliability. 
Models that appear stable under prompt variation may still 
pose significant patient safety risks if their answers are 
systematically incorrect. Safe deployment of AI in clinical 
settings — particularly in low-resource environments where 
human oversight may be limited — requires joint evaluation 
of accuracy, consistency, and instruction adherence.

We call on the research community to move beyond 
accuracy-only evaluation frameworks for clinical AI and 
adopt multidimensional assessment approaches that reflect 
the reliability requirements of real-world healthcare 
deployment. The evaluation framework, datasets, and code 
introduced in this study are publicly available at 
\url{https://github.com/shravani-01/clinical-llm-eval} to support 
reproducibility and future research in this direction.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}

